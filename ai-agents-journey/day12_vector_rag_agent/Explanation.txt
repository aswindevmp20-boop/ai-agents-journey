# Up to now:
# Day 5 ‚Üí keyword RAG
# Day 11 ‚Üí chunked keyword RAG

# Today:
# ‚ùå No keyword matching
# ‚úÖ Semantic meaning
# ‚úÖ Vector similarity
# ‚úÖ Embeddings

# Vector Embeddings :- 
# Vector embeddings are crucial in Retrieval-Augmented Generation (RAG) because they convert text into dense numerical vectors, 
# capturing semantic meaning, which allows RAG systems to perform fast, context-aware similarity searches to find relevant information for LLMs, 
# grounding their responses in external knowledge rather than just internal training data.


# Documents
#    ‚Üì
# Chunking
#    ‚Üì
# Embeddings (vectors)
#    ‚Üì
# Similarity Search (cosine similarity)
#    ‚Üì
# Top-K chunks
#    ‚Üì
# LLM answer grounded in context


To keep this fully local & free:

    sentence-transformers

    Model: all-MiniLM-L6-v2

    Pure Python cosine similarity

    No vector DB yet (FAISS comes Day 13)


pip install sentence-transformers scikit-learn

***************************************************************************************

    Step-by-Step: How the Tool Works Internally

        Let‚Äôs break it into 4 stages.

        ‚ë† Before the tool is ever called (Preprocessing)

            This happens once when the app starts.

            A. Documents are chunked

                Each document is split into small chunks:

                doc1.txt
                ‚îú‚îÄ chunk 1
                ‚îú‚îÄ chunk 2
                ‚îî‚îÄ chunk 3


                Why?

                LLM context is limited

                Smaller chunks = more precise retrieval

            B. Each chunk is converted into a vector (embedding)

                Example (conceptual):

                "Oceans regulate climate"
                ‚Üí [0.021, -0.334, 0.872, ...]  (384 numbers)


                This vector represents meaning, not words.

                So now you have:

                Chunk text  ‚Üí  Vector embedding


                These embeddings are stored in memory.

                  This step happens before any user query.

        ‚ë° User asks a question

            Example:

                "Why are oceans important for climate regulation?"


            At this point:

                LLM does NOT know the answer

                It also does NOT see the documents yet

        ‚ë¢ LLM decides to call the tool

            Because of this system instruction:

                "Always retrieve relevant document chunks before answering."


            The LLM outputs something like:

                {
                "name": "retrieve_chunks",
                "arguments": {
                    "query": "Why are oceans important for climate regulation?",
                    "top_k": 3
                }
                }


             This is the decision point.

             The LLM is saying:

                 ‚ÄúI need external knowledge before I answer.‚Äù

        ‚ë£ The Tool Executes (This is the core)

            Now the Python tool takes over.

            A. Embed the user query
                "Why are oceans important for climate regulation?"
                ‚Üí [0.017, -0.298, 0.901, ...]


            Now both:

                query

                document chunks

                are in the same vector space.

            B. Compare query vector with every chunk vector

                Using cosine similarity:

                Similarity(query, chunk_1) ‚Üí 0.89
                Similarity(query, chunk_2) ‚Üí 0.12
                Similarity(query, chunk_3) ‚Üí 0.76


                This measures semantic closeness, not word overlap.

                Example insight:

                    Query does NOT need the word ‚Äúregulation‚Äù

                    It still matches chunks about climate impact

            C. Rank chunks by similarity
                Ranked results:
                    1Ô∏è‚É£ chunk about ocean heat absorption
                    2Ô∏è‚É£ chunk about CO‚ÇÇ absorption
                    3Ô∏è‚É£ chunk about ocean currents

                Only the top-K are returned.

        ‚ë§ Tool returns context to the LLM

            The tool sends back plain text, not vectors:

            [doc1_oceans.txt]
            Oceans absorb large amounts of heat and carbon dioxide...

            [doc1_oceans.txt]
            Ocean currents distribute heat across the planet...


            This is injected into the conversation as:

                {
                "role": "tool",
                "content": "retrieved context"
                }

        ‚ë• LLM generates the final answer

            Now (and only now), the LLM answers:

            using the retrieved chunks

            grounded in facts

            without hallucinating

            This is Retrieval-Augmented Generation.


***************************************************************************************
DOCUMENT_CHUNKS, CHUNK_EMBEDDINGS = load_and_embed()


DOCUMENT_CHUNKS = [
    {
        "file": "doc1_oceans.txt",
        "content": "Oceans absorb large amounts of heat..."
    },
    {
        "file": "doc1_oceans.txt",
        "content": "Overfishing and pollution threaten marine life..."
    },
    {
        "file": "doc2_ai.txt",
        "content": "Artificial intelligence is transforming industries..."
    }
]


CHUNK_EMBEDDINGS = [
    [0.12, -0.44, 0.78, ...],   # embedding for chunk 1
    [0.05, -0.31, 0.66, ...],   # embedding for chunk 2
    [0.91, -0.22, 0.10, ...],   # embedding for chunk 3
]

Each vector corresponds exactly to the chunk at the same index in DOCUMENT_CHUNKS.

So:

DOCUMENT_CHUNKS[i]  <-->  CHUNK_EMBEDDINGS[i]


This alignment is critical.


**************************************************************************************************
ranked = sorted(
    zip(scores, DOCUMENT_CHUNKS),
    key=lambda x: x[0],
    reverse=True
)

1Ô∏è‚É£ scores

Earlier, you computed:

scores = cosine_similarity([query_embedding], CHUNK_EMBEDDINGS)[0]


This gives you something like:

scores = [0.89, 0.12, 0.76, 0.05]


Each number means:

‚ÄúHow semantically close this chunk is to the query‚Äù

Higher = more relevant.

2Ô∏è‚É£ DOCUMENT_CHUNKS

This is a list of chunk objects:

DOCUMENT_CHUNKS = [
    {"file": "doc1.txt", "content": "Oceans absorb heat..."},
    {"file": "doc1.txt", "content": "Marine ecosystems..."},
    {"file": "doc2.txt", "content": "AI is transforming..."},
    {"file": "doc3.txt", "content": "Traveling allows people..."}
]

3Ô∏è‚É£ zip(scores, DOCUMENT_CHUNKS)

This pairs each score with its corresponding chunk.

Result:

zip(scores, DOCUMENT_CHUNKS)


Becomes (conceptually):

[
  (0.89, {"file": "doc1.txt", "content": "Oceans absorb heat..."}),
  (0.12, {"file": "doc1.txt", "content": "Marine ecosystems..."}),
  (0.76, {"file": "doc2.txt", "content": "AI is transforming..."}),
  (0.05, {"file": "doc3.txt", "content": "Traveling allows people..."})
]


‚ö†Ô∏è Important rule:
This works only because scores[i] corresponds to DOCUMENT_CHUNKS[i].

That alignment was created earlier by:

DOCUMENT_CHUNKS, CHUNK_EMBEDDINGS = load_and_embed()

4Ô∏è‚É£ sorted(...)

Now we sort those (score, chunk) pairs.

sorted(
    zip(scores, DOCUMENT_CHUNKS),
    key=lambda x: x[0],
    reverse=True
)


Let‚Äôs decode this.

5Ô∏è‚É£ key=lambda x: x[0]

Each x looks like:

x = (score, chunk)


So:

x[0] = score
x[1] = chunk


This tells Python:

‚ÄúSort by the score, not the chunk text.‚Äù

6Ô∏è‚É£ reverse=True

By default, sorting is ascending (small ‚Üí big).

But we want:

Highest similarity first

So:

reverse=True


Gives:

[
  (0.89, {...}),
  (0.76, {...}),
  (0.12, {...}),
  (0.05, {...})
]

üèÜ Final result stored in ranked
ranked = [
  (0.89, chunk_about_oceans),
  (0.76, chunk_about_ai),
  (0.12, chunk_about_marine_ecosystems),
  (0.05, chunk_about_travel)
]


Now you can safely do:

top_chunks = ranked[:top_k]


And retrieve only the most relevant knowledge.