DAY 14 — Hybrid RAG (Keyword + Vector Search)

Why Hybrid RAG?

    Pure vector search is great, but it has blind spots:

        Misses exact keywords (IDs, names, acronyms)

        Sometimes retrieves semantically similar but wrong chunks

    Pure keyword search:

        Exact matches

        No semantic understanding

    Hybrid RAG combines both:

        Keyword score (precision)

        Vector score (meaning)

Architecture (Day 14)

        User Query
        ↓
        Keyword Search (BM25-like)
        ↓
        Vector Search (FAISS)
        ↓
        Score Fusion
        ↓
        Top-K Context
        ↓
        LLM Answer

********************************************************************

def retrieve_chunks(query, top_k=TOP_K):
    # vector search
    query_vec =  embedder.encode([query], convert_to_numpy = True)
    distances, indices = index.search(query_vec, top_k * 2)

    candidates = []
    for idx, dist in zip(indices[0], distances[0]):
        vec_score = 1 / (1 + dist)
        key_score = keyword_score(DOCUMENT_CHUNKS[idx]["content"], query)
        final_score = (0.7 * vec_score) + (0.3 * key_score)

        candidates.append((final_score, DOCUMENT_CHUNKS[idx]))

    candidates.sort(key = lambda x:x[0], reverse=True)
    return [chunk for score, chunk in candidates[:top_k]]



The function we’re explaining

    def retrieve_chunks(query, top_k=TOP_K):

    Goal:
        Given a user query, return the top-K most relevant document chunks using BOTH semantic and keyword relevance.

    This is hybrid retrieval.

    1️⃣ Vector search (semantic retrieval)

        query_vec = embedder.encode([query], convert_to_numpy=True)

        What happens here

        The user query (text) is converted into a vector embedding

        This vector represents the meaning of the query

        Example (conceptual):

            "ocean climate regulation"
            → [0.21, -0.44, 0.87, ...]


        This puts the query into the same vector space as document chunks.

        distances, indices = index.search(query_vec, top_k * 2)

        Why top_k * 2?

            FAISS returns the closest vectors based on semantic similarity

            We deliberately fetch more than needed

            Because later we’ll re-rank using keywords

            This gives us a candidate pool instead of final results.

        Example result:

            indices   = [3, 7, 1, 9, 5, 2]
            distances = [0.21, 0.33, 0.45, 0.60, 0.72, 0.91]


        Lower distance = more similar.

    2️⃣ Candidate re-scoring (Hybrid logic)

        candidates = []

        We’ll now re-score each candidate using two signals.

        for idx, dist in zip(indices[0], distances[0]):


        Each iteration:

            idx → index of document chunk

            dist → vector distance for that chunk

            This lets us map FAISS results back to actual text.

    3️⃣ Vector score normalization

        vec_score = 1 / (1 + dist)

        Why this formula?

        FAISS gives distance, but we want similarity.

        This converts:

            Distance	Vector Score
            0.1	           0.91
            0.5	           0.67
            1.0	           0.50

        So:

            Smaller distance → higher score

            All values stay between 0 and 1

            This makes it comparable with keyword scores.

    4️⃣ Keyword score (exact matching)

        key_score = keyword_score(
            DOCUMENT_CHUNKS[idx]["content"],
            query
        )


        This checks:

            How many query words appear exactly

            In the document chunk

        Example:

            Query: "ocean health threats"
            Chunk: "Overfishing is a major threat to ocean ecosystems"

            key_score = 3


        This protects against:

            Missing exact names

            Acronyms

            Technical terms

            IDs

    5️⃣ Hybrid score fusion (the key idea)

        final_score = (0.7 * vec_score) + (0.3 * key_score)

        What this means

        You’re saying:

            “Semantic meaning matters more,
            but keyword accuracy still matters.”

            This weighting is intentional.

            Signal	Weight
            Vector (meaning)	70%
            Keyword (precision)	30%

            This mirrors real production systems.

    6️⃣ Store scored candidates

        candidates.append((final_score, DOCUMENT_CHUNKS[idx]))


        Now you have:

            [
            (0.82, chunk_about_oceans),
            (0.61, chunk_about_climate),
            (0.40, chunk_about_ai)
            ]


        Each chunk is now rankable.

    7️⃣ Final ranking

        candidates.sort(key=lambda x: x[0], reverse=True)


        This sorts by:

            x[0] → final hybrid score

            Highest relevance first

            This is the decisive ranking step.

    8️⃣ Return only top-K chunks

        return [chunk for score, chunk in candidates[:top_k]]


        You discard:

            scores

            low-ranking chunks

            And return only text chunks to the LLM.

        This keeps:

            context clean

            hallucinations low

            answers precise




Think of this function as three filters:

1️⃣ FAISS → “Which chunks might be relevant?”
2️⃣ Keyword scoring → “Which chunks are exact matches?”
3️⃣ Hybrid ranking → “Which chunks deserve to be shown?”

Only the best survive.